APACHE BEAM
===========

Apache Beam is a framework for defining both batch and streaming data-processing pipelines.

You can think of Beam as an additional layer of abstraction on top of an underlying "distributed processing back-end":

* Apache Apex
* Apache Flink
* Apache Spark
* Google Cloud Dataflow

Creating the pipeline
---------------------

1. `PipelineOptions`

`PipelineOptions` defines the options for your Beam pipeline. Common options include:

* The pipeline runner that will execute this pipeline (more on runners later)
* If you're using Google Dataflow, the project to run the pipeline in
* The pipeline input and output destinations (either files, tables, or GCS buckets)

`PipelineOptions` may be hard-coded into your pipeline code, or more often, passed as CLI arguments.

```
PipelineOptions options = PipelineOptionsFactory.create();
```

2. `Pipeline`

The next step is to create a `Pipeline` object with the options we've just constructed.

The `Pipeline` creates a graph of transformations to be executed.

```
Pipeline p = Pipeline.create(options)
```

Applying pipeline transformations
---------------------------------

A pipeline takes some input, applies one or more transformations, and writes the results to some output.'

In this doc, we'll walk through an example pipeline that generates word counts from the works of Shakespeare.

1. `PCollection`

This is the "stuff" of your transformation. The input and output data are usually represented as  `PCollection`s.

`PCollection` are abstract enough to represent almost any dataset, including unbounded datasets.

2. Reading from a file

Our first step in our example pipeline is to read the works of Shakespeare from a file.

Yep, this returns a `PCollection`:

```
p.apply(TextIO.read().from("gs://apache-beam-samples/shakespeare/*")) 
```

3. Split into lines

In this step, we split the text into lines, where each element is a individual word.

In other words, we're taking a `PCollection` of lines, and turning it into a `PCollection` of words via a `FlatMapElements` transform:

```
.apply("ExtractWords", FlatMapElements
        .into(TypeDescriptors.strings())
        .via((String word) -> Arrays.asList(word.split("[^\\p{L}]+"))))
```

4. Counting the words

The Beam SKD provides a `Count` transform, which takes a `PCollection` of any type, and returns a `PCollection` of key-value pairs.

Each key is a unique element in the original `PCollection` (in this example, a word), and each value is the number of times that element occurs in the collection.

```
.apply(Count.<String>perElement())
```

5. Filtering out empty words

We can filter out empty words using the `Filter` transform:

```
.apply(Filter.by((String word) -> !word.isEmpty())
```

6. Transforming the counts

The next transform simple formats our key-value pairs into something human-readable, `Word: count`.

This is a simple `MapElements` transform:

```
.apply("FormatResults", MapElements
    .into(TypeDescriptors.strings())
    .via((KV<String, Long> wordCount) -> wordCount.getKey() + ": " + wordCount.getValue()))
```

7. Output to a file

Our final transformation is to write our output to a file:


```
.apply(TextIO.write().to("wordcounts"));
```

Running the pipeline
--------------------

In the previous section, we defined a word count pipeline that looks like this:

```
p.apply(TextIO.read().from("gs://apache-beam-samples/shakespeare/*"))
    .apply(FlatMapElements
        .into(TypeDescriptors.strings())
        .via((String word) -> Arrays.asList(word.split("[^\\p{L}]+"))))
    .apply(Filter.by((String word) -> !word.isEmpty()))
    .apply(Count.perElement())
    .apply(MapElements
        .into(TypeDescriptors.strings())
        .via((KV<String, Long> wordCount) -> wordCount.getKey() + ": " + wordCount.getValue()))
    .apply(TextIO.write().to("wordcounts"));
```

We can run it with this simple command:

```
p.run().waitUntilFinish();
```

